{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string, random\n",
    "def extract_data():\n",
    "    with open('DataSet/test.csv', 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "\n",
    "        with open('DataSet/dataset1.csv', 'w', newline='') as new_file:\n",
    "            csv_writer = csv.writer(new_file, delimiter=',')\n",
    "            csv_headings = next(csv_reader)\n",
    "            for i in range(9+1):\n",
    "                i = csv_headings\n",
    "                csv_headings = next(csv_reader)\n",
    "                csv_writer.writerow(i)\n",
    "extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_special_char():\n",
    "    dict_data = {}\n",
    "    df = pd.read_csv('DataSet/dataset1.csv', delimiter=\",\")\n",
    "    spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "                  \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "                  \"_\",\"~\",\">\",\"?\",\"@\",\"[\",\"]\",\"\\\\\",\"{\",\n",
    "                  \"}\",\"^\",\"`\",\"|\",\"br \"]\n",
    "\n",
    "    for char in spec_chars:\n",
    "        df['review'] = df['review'].str.replace(char, ' ')\n",
    "        df['review'] = df['review'].str.split().str.join(' ')\n",
    "        df['sentiment'] = df['sentiment'].str.replace(char, ' ')\n",
    "        df['sentiment'] = df['sentiment'].str.split().str.join(' ')\n",
    "        dict_data['review']=df['review']\n",
    "        dict_data['sentiment']=df['sentiment']\n",
    "\n",
    "    df = pd.DataFrame(dict_data)\n",
    "    df.to_csv('DataSet/dataset2.csv', \",\", header=True)\n",
    "\n",
    "clean_special_char()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AliR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import io \n",
    "import codecs\n",
    "import re, string, random\n",
    "nltk.download('punkt')\n",
    "\n",
    "input_str = \"\"\n",
    "new_str = \"\"\n",
    "with open('DataSet/dataset2.csv', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        input_str = input_str + line\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(input_str)\n",
    "    filtered_words = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "def listToString(s):  \n",
    "    str1 = \" \"    \n",
    "    return (str1.join(s))\n",
    "\n",
    "    p = listToString(filtered_words)\n",
    "    print(p)\n",
    "    sys.exit(0)\n",
    "    sentiment_dict = sid_obj.polarity_scores(p) \n",
    "    print(sentiment_dict)\n",
    "    if sentiment_dict['compound'] >= 0.05 : \n",
    "        print(\"Positive\") \n",
    "    elif sentiment_dict['compound'] <= - 0.05 : \n",
    "        print(\"Negative\") \n",
    "    else :\n",
    "        print(\"Neutral\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import html\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&.{4};')\n",
    "    cleantext = re.sub(cleanr, '', str(raw_html))\n",
    "#     replacing the special characters\n",
    "#     cleanr = re.compile ('\\\\n')\n",
    "#     cleantext = re.sub(cleanr, ' ', cleantext)\n",
    "    clean = re.sub('\\s+',' ',cleantext)\n",
    "    return html.unescape(clean) # replaces the special characters\n",
    "\n",
    "#same using beautifulsoup\n",
    "\n",
    "def remove_html_escape(html):\n",
    "    return BeautifulSoup(str(html), \"lxml\").text\n",
    "\n",
    "file = input(\"Enter CSV File name (without '.csv' at the end): \")\n",
    "\n",
    "# reading the file\n",
    "try:\n",
    "    d = pd.read_csv(\"%s.csv\" % file )\n",
    "except IOError:\n",
    "    print (\"Error: can\\'t find file or read data\")\n",
    "else:\n",
    "    print (\"File read successfully\")\n",
    "\n",
    "a = pd.DataFrame(d)\n",
    "print(\"File preview: \\n\",a.head(5))\n",
    "\n",
    "col = input(\"Enter column name: \")\n",
    "\n",
    "try: \n",
    "    a[col][0:2]\n",
    "except:\n",
    "    print(\"Error in fetching column. Please check the name '%s' from the table preview above\" %col)\n",
    "else:\n",
    "    print(\"Column read successfully: \\n\", a[col][0:5])\n",
    "\n",
    "a['clean'] = a[col].apply(cleanhtml)\n",
    "\n",
    "# a['clean_bs'] = a['question'].apply(remove_html_escape)\n",
    "\n",
    "\n",
    "a['parity'] = a[col].str.len() - a['clean'].str.len() #using regex\n",
    "# a['parity_bs'] = a[col].str.len() - a['clean_bs'].str.len() #using beautifulsoup\n",
    "\n",
    "# a.tail(5)\n",
    "\n",
    "print (\"------------------------------------------------- \\n HTML has been removed from your column contents \\n------------------------------------------------- \\n \")\n",
    "print (\"column 'clean' contins regex replacement of anything in between < > or &; or \\\\* \\nin otherwords, it removes any html with the space character, no conversion of special characters to respective ASCII values.\")\n",
    "print (\"column 'clean_bs' contains html removed with special characters replaced with their respective ASCII characters.\")\n",
    "print (\"Parity columns show the difference in number of characters from the original html\")\n",
    "\n",
    "print (\"Output table: \\n %s\" %a.head(5))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "a.to_csv(\"%shtml_cleaned_output.csv\"%file)\n",
    "print(\"New file '%shtml_cleaned_output.csv' generated with cleaned columns. Check in the same direcotry\"%file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data():\n",
    "# #     df = pd.read_csv('DataSet/dataset1.csv')\n",
    "#     spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "#               \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "#               \"_\",\"~\",\">\",\"?\",\"@\",\"[\",\"]\",\"\\\\\",\"{\",\n",
    "#               \"}\",\"^\",\"`\",\"|\",\"br \"]\n",
    "    \n",
    "#     with open('DataSet/dataset1.csv', 'r') as csvfile:\n",
    "#         csv_reader = csv.reader(csvfile)\n",
    "        \n",
    "#         with open('DataSet/dataset2.csv', 'w', newline='') as new_file:\n",
    "#             csv_writer = csv.writer(new_file, delimiter=',')\n",
    "            \n",
    "#             for k in csv_reader.split(\"\\n\"):\n",
    "#                 print(re.sub(r\"[^a-zA-Z0-9]+\", ' ', k))\n",
    "#                 break\n",
    "#             for i in csv_reader,spec_chars:\n",
    "#                 new_str = re.sub('[^a-zA-Z0-9\\n\\.]', '', i)\n",
    "#                 print(new_str)\n",
    "# #             token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "# #                        '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', csv_reader)\n",
    "# #             token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", csv_reader)\n",
    "#             sys.exit(0)\n",
    "#             for i in new_str:\n",
    "#                 csv_writer.writerow(new_str)\n",
    "                \n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&.{4};')\n",
    "    cleantext = re.sub(cleanr, '', str(raw_html))\n",
    "#     replacing the special characters\n",
    "#     cleanr = re.compile ('\\\\n')\n",
    "#     cleantext = re.sub(cleanr, ' ', cleantext)\n",
    "    clean = re.sub('\\s+',' ',cleantext)\n",
    "    return html.unescape(clean) # replaces the special characters\n",
    "\n",
    "def remove_html_escape(html):\n",
    "    return BeautifulSoup(str(html), \"lxml\").text\n",
    "\n",
    "d = pd.read_csv(\"%s.csv\" % file )\n",
    "\n",
    "a = pd.DataFrame(d)\n",
    "# print(\"File preview: \\n\",a.head(5))\n",
    "\n",
    "a['clean'] = a.apply(cleanhtml)\n",
    "\n",
    "\n",
    "a['clean_bs'] = a['question'].apply(remove_html_escape)\n",
    "\n",
    "a.to_csv(\"%snew.csv\"%file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     new_str = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', csv_reader)\n",
    "#     print(new_str)\n",
    "#     open('DataSet/dataset2.csv', 'w', newline='').write(new_str)\n",
    "\n",
    "#     regex_pat = re.compile(r'FUZ', flags=re.IGNORECASE)\n",
    "#     pd.Series(['foo', 'fuz', np.nan]).str.replace(regex_pat, ' ')\n",
    "# #     print(regex_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my.to_csv(\"%snew.csv\"%file)\n",
    "# with open('DataSet/dataset1.csv', 'r') as csvfile:\n",
    "#     csv_reader = csv.reader(csvfile)\n",
    "        \n",
    "#     with open('DataSet/dataset2.csv', 'w', newline='') as new_file:\n",
    "#         csv_writer = csv.writer(new_file, delimiter=',')\n",
    "#         for i in csv_reader:            \n",
    "#             new_str = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', i)\n",
    "#             print(new_str)\n",
    "#             open('DataSet/dataset2.csv', 'w', newline='').write(new_str)\n",
    "    \n",
    "        \n",
    "#     string = \"Special $#! characters   spaces 888323\"\n",
    "#     new_string = string.split('')\n",
    "#     print(''.join(e for e in string if e.isalnum()))\n",
    "#     print(review.value_counts(dropna=False))\n",
    "with open('DataSet/dataset2.csv', 'w', newline='') as new_file:\n",
    "    fieldnames = ['review', 'sentiment']\n",
    "    writer = csv.DictWriter(new_file, fieldnames=fieldnames)\n",
    "    for char in spec_chars:\n",
    "        df['review'] = df['review'].str.replace(char, ' ')\n",
    "        df['review'] = df['review'].str.split().str.join(' ')\n",
    "        df['sentiment'] = df['sentiment'].str.replace(char, ' ')\n",
    "        df['sentiment'] = df['sentiment'].str.split().str.join(' ')\n",
    "    writer.writeheader()\n",
    "    writer.writerow({fieldnames : df['review'], fieldnames: df['sentiment']})\n",
    "    print(df['review'],df['sentiment'])\n",
    "    sys.exit(0)\n",
    "\n",
    "# data = np.array([])\n",
    "# with open('DataSet/dataset1.csv', 'rb') as csvfile: \n",
    "#     reader = csv.reader(csvfile, skipinitialspace=True)\n",
    "#     data.append(str(next(reader)))\n",
    "#     for num, val in reader:\n",
    "#         data.append((int(num), val))\n",
    "# print(data)\n",
    "# numpy_array = np.genfromtxt(\"DataSet/dataset1.csv\", delimiter=\"\\t\", skip_header=1)\n",
    "# print(numpy_array)\n",
    "\n",
    "# with open(\"DataSet/dataset1.csv\", 'r') as f:\n",
    "#     wines = list(csv.reader(f, delimiter=\";\"))\n",
    "\n",
    "# import pandas\n",
    "# dfdict={}\n",
    "# dfdict[\"a\"]=[1,2,3,4]\n",
    "# dfdict[\"b\"]=[5,6,7,8]\n",
    "# dfdict[\"c\"]=[9,10,11,12]\n",
    "# df=pandas.DataFrame(dfdict)\n",
    "# df.to_csv(\"dfTest.txt\",\"\\t\",header=True,cols=[\"b\",\"a\",\"c\"])\n",
    "\n",
    "# df.to_csv(index=False)\n",
    "\n",
    "# wines = np.array(wines)\n",
    "# for i in wines:\n",
    "#     modified_string = re.sub(r'spec_chars', '', i) # on individual tokens \n",
    "#     modified_string = re.sub(r'spec_chars', '', i)\n",
    "#     print(modified_string)\n",
    "\n",
    "# with open('DataSet/dataset1.csv', 'r') as csvfile:\n",
    "#     csv_reader = csv.reader(csvfile)\n",
    "#     modified_string = re.sub(r'\\W+', '', csv_reader) # on individual tokens \n",
    "#     modified_string = re.sub(r'[^a-zA-Z0-9_\\s]+', '', csv_reader)\n",
    "#     print(modified_string)\n",
    "#     sys.exit(0)\n",
    "#     open('DataSet/dataset2.csv', 'w', newline='').write(new_str)\n",
    "\n",
    "# for char in spec_chars:\n",
    "#     df['review'] = df['review'].str.replace(char, ' ')\n",
    "#     df['review'] = df['review'].str.split().str.join(' ')\n",
    "#     df['sentiment'] = df['sentiment'].str.replace(char, ' ')\n",
    "#     df['sentiment'] = df['sentiment'].str.split().str.join(' ')\n",
    "# #     print(df['review'])\n",
    "# my = df.to_dict('index')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
